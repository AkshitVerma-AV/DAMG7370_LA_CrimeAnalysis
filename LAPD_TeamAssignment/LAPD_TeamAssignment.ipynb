{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c9d43a82-b36a-4cd3-a9d2-6aa9cc8c50dd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pyspark.pipelines as dp\n",
    "import pyspark.sql.functions as sf\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "# ============================================\n",
    "# BRONZE LAYER\n",
    "# ============================================\n",
    "\n",
    "@dp.table()\n",
    "def lapd_bronze():\n",
    "    df = spark.read \\\n",
    "        .format(\"csv\") \\\n",
    "        .option(\"header\", \"true\") \\\n",
    "        .option(\"inferSchema\", \"false\") \\\n",
    "        .load(\"/Volumes/workspace/damg7370/lapd/Cleaned_LAPD.csv\")\n",
    "    \n",
    "    # Force correct types\n",
    "    df = df.withColumn(\"DR_NO\", sf.col(\"DR_NO\").cast(\"string\"))\n",
    "    df = df.withColumn(\"TIME_OCC\", sf.trim(sf.col(\"TIME_OCC\")).cast(\"string\"))\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c7f453ac-78c0-4115-b61c-57ff8373a8d3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# SILVER LAYER\n",
    "# ============================================\n",
    "\n",
    "@dp.table()\n",
    "@dp.expect_or_drop(\"dr_no_nn\", \"DR_NO is not null\")\n",
    "def lapd_silver():\n",
    "    df = (spark.readStream\n",
    "          .option(\"skipChangeCommits\", \"true\")\n",
    "          .table('lapd_bronze'))\n",
    "    \n",
    "    # Dates are already in correct format, just ensure they're date type\n",
    "    df = df.withColumn(\"DATE_RPTD\", sf.col(\"DATE_RPTD\").cast(\"date\"))\n",
    "    df = df.withColumn(\"DATE_OCC\", sf.col(\"DATE_OCC\").cast(\"date\"))\n",
    "    \n",
    "    # TIME FIX\n",
    "    df = df.withColumn(\"TIME_OCC\", sf.col(\"TIME_OCC\"))\n",
    "    df = df.withColumn(\"TIME_OCC_HOUR\", \n",
    "        sf.expr(\"try_cast(substr(TIME_OCC,1,2) AS INT)\"))\n",
    "    \n",
    "    # Add required columns\n",
    "    df = df.withColumn(\"INCIDENT_COUNT\", sf.lit(1))\n",
    "    df = df.withColumn(\"last_updated\", sf.current_timestamp())\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9e778fcf-ffbf-4b32-8057-057ff41972ca",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# DIMENSION TABLES\n",
    "# ============================================\n",
    "\n",
    "# DIM_DATE\n",
    "dp.create_streaming_table(\n",
    "    name=\"dim_date_lapd\",\n",
    "    schema=\"\"\"\n",
    "        DATE_KEY bigint generated always as identity (start with 1 increment by 1),\n",
    "        FULL_DATE date,\n",
    "        YEAR int,\n",
    "        QUARTER_NUMBER int,\n",
    "        MONTH_NUMBER int,\n",
    "        MONTH_NAME string,\n",
    "        DAY_OF_MONTH int,\n",
    "        DAY_OF_WEEK int,\n",
    "        DAY_OF_WEEK_NAME string,\n",
    "        last_updated timestamp\n",
    "    \"\"\",\n",
    "    table_properties={\"delta.enableChangeDataFeed\": \"true\"}\n",
    ")\n",
    "\n",
    "@dp.view()\n",
    "def gold_dim_date():\n",
    "    df = spark.readStream.table('lapd_silver')\n",
    "    \n",
    "    df = df.selectExpr(\n",
    "        \"DATE_OCC as FULL_DATE\",\n",
    "        \"year(DATE_OCC) as YEAR\",\n",
    "        \"quarter(DATE_OCC) as QUARTER_NUMBER\",\n",
    "        \"month(DATE_OCC) as MONTH_NUMBER\",\n",
    "        \"date_format(DATE_OCC, 'MMMM') as MONTH_NAME\",\n",
    "        \"dayofmonth(DATE_OCC) as DAY_OF_MONTH\",\n",
    "        \"dayofweek(DATE_OCC) as DAY_OF_WEEK\",\n",
    "        \"date_format(DATE_OCC, 'EEEE') as DAY_OF_WEEK_NAME\",\n",
    "        \"last_updated\"\n",
    "    )\n",
    "    \n",
    "    return df\n",
    "\n",
    "dp.create_auto_cdc_flow(\n",
    "    target=\"dim_date_lapd\",\n",
    "    source=\"gold_dim_date\",\n",
    "    keys=[\"FULL_DATE\"],\n",
    "    sequence_by=\"last_updated\",\n",
    "    ignore_null_updates=True\n",
    ")\n",
    "\n",
    "# DIM_TIME\n",
    "dp.create_streaming_table(\n",
    "    name=\"dim_time_lapd\",\n",
    "    schema=\"\"\"\n",
    "        TIME_KEY bigint generated always as identity (start with 1 increment by 1),\n",
    "        HOUR int,\n",
    "        MINUTE int,\n",
    "        TIME_BAND string,\n",
    "        last_updated timestamp\n",
    "    \"\"\",\n",
    "    table_properties={\"delta.enableChangeDataFeed\": \"true\"}\n",
    ")\n",
    "\n",
    "@dp.view()\n",
    "def gold_dim_time():\n",
    "    df = spark.readStream.table('lapd_silver')\n",
    "    \n",
    "    df = df.filter(sf.col(\"TIME_OCC\").isNotNull() & (sf.col(\"TIME_OCC\") != \"\"))\n",
    "    df = df.withColumn(\"HOUR\", sf.split(sf.col(\"TIME_OCC\"), \":\")[0].cast(\"int\"))\n",
    "    df = df.withColumn(\"MINUTE\", sf.split(sf.col(\"TIME_OCC\"), \":\")[1].cast(\"int\"))\n",
    "    \n",
    "    df = df.withColumn(\"TIME_BAND\",\n",
    "        sf.when((sf.col(\"HOUR\") >= 5) & (sf.col(\"HOUR\") < 12), \"Morning\")\n",
    "        .when((sf.col(\"HOUR\") >= 12) & (sf.col(\"HOUR\") < 17), \"Afternoon\")\n",
    "        .when((sf.col(\"HOUR\") >= 17) & (sf.col(\"HOUR\") < 21), \"Evening\")\n",
    "        .otherwise(\"Night\"))\n",
    "    \n",
    "    df = df.selectExpr(\"HOUR\", \"MINUTE\", \"TIME_BAND\", \"last_updated\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "dp.create_auto_cdc_flow(\n",
    "    target=\"dim_time_lapd\",\n",
    "    source=\"gold_dim_time\",\n",
    "    keys=[\"HOUR\", \"MINUTE\"],\n",
    "    sequence_by=\"last_updated\",\n",
    "    ignore_null_updates=True\n",
    ")\n",
    "\n",
    "# DIM_AREA\n",
    "dp.create_streaming_table(\n",
    "    name=\"dim_area_lapd\",\n",
    "    schema=\"\"\"\n",
    "        AREA_KEY bigint generated always as identity (start with 1 increment by 1),\n",
    "        AREA_CODE string,\n",
    "        AREA_NAME string,\n",
    "        BUREAU_NAME string,\n",
    "        last_updated timestamp\n",
    "    \"\"\",\n",
    "    table_properties={\"delta.enableChangeDataFeed\": \"true\"}\n",
    ")\n",
    "\n",
    "@dp.view()\n",
    "def gold_dim_area():\n",
    "    df = spark.readStream.table('lapd_silver')\n",
    "    \n",
    "    df = df.withColumn(\"AREA_CODE\", sf.col(\"AREA\").cast(\"string\"))\n",
    "    df = df.withColumn(\"BUREAU_NAME\",\n",
    "        sf.when(sf.col(\"AREA\").isin(1,2,3,4,5,6), \"Central Bureau\")\n",
    "        .when(sf.col(\"AREA\").isin(7,8,9,10,17), \"Valley Bureau\")\n",
    "        .when(sf.col(\"AREA\").isin(11,14,15,16), \"West Bureau\")\n",
    "        .when(sf.col(\"AREA\").isin(12,13,18), \"South Bureau\")\n",
    "        .otherwise(\"Unknown Bureau\"))\n",
    "    \n",
    "    df = df.selectExpr(\"AREA_CODE\", \"AREA_NAME\", \"BUREAU_NAME\", \"last_updated\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "dp.create_auto_cdc_flow(\n",
    "    target=\"dim_area_lapd\",\n",
    "    source=\"gold_dim_area\",\n",
    "    keys=[\"AREA_CODE\"],\n",
    "    sequence_by=\"last_updated\",\n",
    "    ignore_null_updates=True\n",
    ")\n",
    "\n",
    "# DIM_STATUS\n",
    "dp.create_streaming_table(\n",
    "    name=\"dim_status_lapd\",\n",
    "    schema=\"\"\"\n",
    "        STATUS_KEY bigint generated always as identity (start with 1 increment by 1),\n",
    "        STATUS_CODE string,\n",
    "        STATUS_DESC string,\n",
    "        last_updated timestamp\n",
    "    \"\"\",\n",
    "    table_properties={\"delta.enableChangeDataFeed\": \"true\"}\n",
    ")\n",
    "\n",
    "@dp.view()\n",
    "def gold_dim_status():\n",
    "    df = spark.readStream.table('lapd_silver')\n",
    "    df = df.selectExpr(\"STATUS as STATUS_CODE\", \"STATUS_DESC\", \"last_updated\")\n",
    "    return df\n",
    "\n",
    "dp.create_auto_cdc_flow(\n",
    "    target=\"dim_status_lapd\",\n",
    "    source=\"gold_dim_status\",\n",
    "    keys=[\"STATUS_CODE\"],\n",
    "    sequence_by=\"last_updated\",\n",
    "    ignore_null_updates=True\n",
    ")\n",
    "\n",
    "# DIM_WEAPON\n",
    "dp.create_streaming_table(\n",
    "    name=\"dim_weapon_lapd\",\n",
    "    schema=\"\"\"\n",
    "        WEAPON_KEY bigint generated always as identity (start with 1 increment by 1),\n",
    "        WEAPON_USED_CD string,\n",
    "        WEAPON_DESC string,\n",
    "        WEAPON_GROUP string,\n",
    "        last_updated timestamp\n",
    "    \"\"\",\n",
    "    table_properties={\"delta.enableChangeDataFeed\": \"true\"}\n",
    ")\n",
    "\n",
    "@dp.view()\n",
    "def gold_dim_weapon():\n",
    "    df = spark.readStream.table('lapd_silver')\n",
    "    \n",
    "    df = df.withColumn(\"WEAPON_GROUP\",\n",
    "        sf.when(sf.col(\"WEAPON_DESC\").like(\"%GUN%\"), \"Firearm\")\n",
    "        .when(sf.col(\"WEAPON_DESC\").like(\"%KNIFE%\"), \"Sharp Object\")\n",
    "        .when(sf.col(\"WEAPON_DESC\").like(\"%BLUNT%\"), \"Blunt Object\")\n",
    "        .when(sf.col(\"WEAPON_DESC\").like(\"%STRANGLE%\"), \"Strangulation\")\n",
    "        .otherwise(\"Other / Unknown\"))\n",
    "    \n",
    "    df = df.selectExpr(\"WEAPON_USED_CD\", \"WEAPON_DESC\", \"WEAPON_GROUP\", \"last_updated\")\n",
    "    return df\n",
    "\n",
    "dp.create_auto_cdc_flow(\n",
    "    target=\"dim_weapon_lapd\",\n",
    "    source=\"gold_dim_weapon\",\n",
    "    keys=[\"WEAPON_USED_CD\"],\n",
    "    sequence_by=\"last_updated\",\n",
    "    ignore_null_updates=True\n",
    ")\n",
    "\n",
    "# DIM_PREMISE\n",
    "dp.create_streaming_table(\n",
    "    name=\"dim_premise_lapd\",\n",
    "    schema=\"\"\"\n",
    "        PREMISE_KEY bigint generated always as identity (start with 1 increment by 1),\n",
    "        PREMIS_CD string,\n",
    "        PREMIS_DESC string,\n",
    "        PREMISE_CATEGORY string,\n",
    "        last_updated timestamp\n",
    "    \"\"\",\n",
    "    table_properties={\"delta.enableChangeDataFeed\": \"true\"}\n",
    ")\n",
    "\n",
    "@dp.view()\n",
    "def gold_dim_premise():\n",
    "    df = spark.readStream.table('lapd_silver')\n",
    "    \n",
    "    df = df.withColumn(\"PREMISE_CATEGORY\",\n",
    "        sf.when(sf.col(\"PREMIS_DESC\").rlike(\"(?i)PARKING|GARAGE\"), \"Outside\")\n",
    "        .when(sf.col(\"PREMIS_DESC\").rlike(\"(?i)SINGLE|MULTI|APARTMENT|RESIDENCE|HOME\"), \"Residence\")\n",
    "        .when(sf.col(\"PREMIS_DESC\").rlike(\"(?i)STORE|MARKET|SHOP|BUSINESS|OFFICE|BANK\"), \"Commercial\")\n",
    "        .otherwise(\"Other\"))\n",
    "    \n",
    "    df = df.selectExpr(\"PREMIS_CD\", \"PREMIS_DESC\", \"PREMISE_CATEGORY\", \"last_updated\")\n",
    "    return df\n",
    "\n",
    "dp.create_auto_cdc_flow(\n",
    "    target=\"dim_premise_lapd\",\n",
    "    source=\"gold_dim_premise\",\n",
    "    keys=[\"PREMIS_CD\"],\n",
    "    sequence_by=\"last_updated\",\n",
    "    ignore_null_updates=True\n",
    ")\n",
    "\n",
    "# DIM_CRIME\n",
    "dp.create_streaming_table(\n",
    "    name=\"dim_crime_lapd\",\n",
    "    schema=\"\"\"\n",
    "        CRIME_KEY bigint generated always as identity (start with 1 increment by 1),\n",
    "        CRM_CD_1 string,\n",
    "        CRM_CD_2 string,\n",
    "        CRM_CD_3 string,\n",
    "        CRM_CD_4 string,\n",
    "        CRM_CD_DESC string,\n",
    "        PART_1_2 string,\n",
    "        CRIME_CATEGORY string,\n",
    "        CRIME_SUBCATEGORY string,\n",
    "        last_updated timestamp\n",
    "    \"\"\",\n",
    "    table_properties={\"delta.enableChangeDataFeed\": \"true\"}\n",
    ")\n",
    "\n",
    "@dp.view()\n",
    "def gold_dim_crime():\n",
    "    df = spark.readStream.table('lapd_silver')\n",
    "    \n",
    "    df = df.withColumn(\"CRIME_CATEGORY\",\n",
    "        sf.when(sf.col(\"CRM_CD_DESC\").rlike(\"(?i)ASSAULT|BATTERY\"), \"Assault & Battery\")\n",
    "        .when(sf.col(\"CRM_CD_DESC\").rlike(\"(?i)ROBBERY\"), \"Robbery\")\n",
    "        .when(sf.col(\"CRM_CD_DESC\").rlike(\"(?i)BURGLARY\"), \"Burglary\")\n",
    "        .when(sf.col(\"CRM_CD_DESC\").rlike(\"(?i)THEFT|LARCENY|SHOPLIFTING\"), \"Theft & Larceny\")\n",
    "        .when(sf.col(\"CRM_CD_DESC\").rlike(\"(?i)FRAUD|ID THEFT\"), \"Fraud & Identity\")\n",
    "        .when(sf.col(\"CRM_CD_DESC\").rlike(\"(?i)HOMICIDE|MURDER\"), \"Homicide\")\n",
    "        .when(sf.col(\"CRM_CD_DESC\").rlike(\"(?i)SEXUAL\"), \"Sexual Crimes\")\n",
    "        .when(sf.col(\"CRM_CD_DESC\").rlike(\"(?i)ARSON\"), \"Arson\")\n",
    "        .when(sf.col(\"CRM_CD_DESC\").rlike(\"(?i)WEAPON\"), \"Weapons Offense\")\n",
    "        .otherwise(\"Other / Miscellaneous\"))\n",
    "    \n",
    "    df = df.withColumn(\"CRIME_SUBCATEGORY\",\n",
    "        sf.when(sf.col(\"CRM_CD_DESC\").rlike(\"(?i)SIMPLE ASSAULT\"), \"Simple Assault\")\n",
    "        .when(sf.col(\"CRM_CD_DESC\").rlike(\"(?i)AGGRAVATED ASSAULT\"), \"Aggravated Assault\")\n",
    "        .when(sf.col(\"CRM_CD_DESC\").rlike(\"(?i)STRONG-ARM\"), \"Strong-Arm Robbery\")\n",
    "        .when(sf.col(\"CRM_CD_DESC\").rlike(\"(?i)BURGLARY\"), \"Burglary\")\n",
    "        .when(sf.col(\"CRM_CD_DESC\").rlike(\"(?i)AUTO THEFT|VEHICLE - STOLEN\"), \"Auto Theft\")\n",
    "        .when(sf.col(\"CRM_CD_DESC\").rlike(\"(?i)SHOPLIFTING\"), \"Shoplifting\")\n",
    "        .when(sf.col(\"CRM_CD_DESC\").rlike(\"(?i)FRAUD|ID THEFT\"), \"Fraud / Identity Theft\")\n",
    "        .otherwise(\"General Offense\"))\n",
    "    \n",
    "    df = df.selectExpr(\"CRM_CD_1\", \"CRM_CD_2\", \"CRM_CD_3\", \"CRM_CD_4\", \"CRM_CD_DESC\", \"PART_1_2\", \"CRIME_CATEGORY\", \"CRIME_SUBCATEGORY\", \"last_updated\")\n",
    "    return df\n",
    "\n",
    "dp.create_auto_cdc_flow(\n",
    "    target=\"dim_crime_lapd\",\n",
    "    source=\"gold_dim_crime\",\n",
    "    keys=[\"CRM_CD_1\"],\n",
    "    sequence_by=\"last_updated\",\n",
    "    ignore_null_updates=True\n",
    ")\n",
    "\n",
    "# DIM_MODUS_OPERANDI\n",
    "dp.create_streaming_table(\n",
    "    name=\"dim_modus_operandi_lapd\",\n",
    "    schema=\"\"\"\n",
    "        MODUS_OPERANDI_KEY bigint generated always as identity (start with 1 increment by 1),\n",
    "        MOCODES string,\n",
    "        MO_DESC string,\n",
    "        MO_TYPE string,\n",
    "        last_updated timestamp\n",
    "    \"\"\",\n",
    "    table_properties={\"delta.enableChangeDataFeed\": \"true\"}\n",
    ")\n",
    "\n",
    "@dp.view()\n",
    "def gold_dim_modus_operandi():\n",
    "    df = spark.readStream.table('lapd_silver')\n",
    "    \n",
    "    df = df.filter(sf.col(\"MOCODES\").isNotNull())\n",
    "    \n",
    "    df = df.withColumn(\"MO_DESC\",\n",
    "        sf.when(sf.col(\"MOCODES\").like(\"%00%\"), \"General Behavior\")\n",
    "        .when(sf.col(\"MOCODES\").like(\"%10%\"), \"Threat or Intimidation\")\n",
    "        .when(sf.col(\"MOCODES\").like(\"%20%\"), \"Physical Force\")\n",
    "        .when(sf.col(\"MOCODES\").like(\"%30%\"), \"Weapon Use\")\n",
    "        .when(sf.col(\"MOCODES\").like(\"%40%\"), \"Entry or Break-in\")\n",
    "        .otherwise(\"Other / Unknown\"))\n",
    "    \n",
    "    df = df.withColumn(\"MO_TYPE\",\n",
    "        sf.when(sf.col(\"MO_DESC\").contains(\"Threat\"), \"Threatening\")\n",
    "        .when(sf.col(\"MO_DESC\").contains(\"Force\"), \"Force\")\n",
    "        .when(sf.col(\"MO_DESC\").contains(\"Weapon\"), \"Weapon-related\")\n",
    "        .when(sf.col(\"MO_DESC\").contains(\"Entry\"), \"Entry-related\")\n",
    "        .otherwise(\"General\"))\n",
    "    \n",
    "    df = df.selectExpr(\"MOCODES\", \"MO_DESC\", \"MO_TYPE\", \"last_updated\")\n",
    "    return df\n",
    "\n",
    "dp.create_auto_cdc_flow(\n",
    "    target=\"dim_modus_operandi_lapd\",\n",
    "    source=\"gold_dim_modus_operandi\",\n",
    "    keys=[\"MOCODES\"],\n",
    "    sequence_by=\"last_updated\",\n",
    "    ignore_null_updates=True\n",
    ")\n",
    "\n",
    "# DIM_LOCATION\n",
    "dp.create_streaming_table(\n",
    "    name=\"dim_location_lapd\",\n",
    "    schema=\"\"\"\n",
    "        LOCATION_KEY bigint generated always as identity (start with 1 increment by 1),\n",
    "        RPT_DIST_NO string,\n",
    "        AREA string,\n",
    "        LOCATION string,\n",
    "        CROSS_STREET string,\n",
    "        LAT double,\n",
    "        LON double,\n",
    "        last_updated timestamp\n",
    "    \"\"\",\n",
    "    table_properties={\"delta.enableChangeDataFeed\": \"true\"}\n",
    ")\n",
    "\n",
    "@dp.view()\n",
    "def gold_dim_location():\n",
    "    df = spark.readStream.table('lapd_silver')\n",
    "    \n",
    "    # CAST LAT and LON to double (THIS IS THE FIX)\n",
    "    df = df.withColumn(\"LAT\", sf.col(\"LAT\").cast(\"double\"))\n",
    "    df = df.withColumn(\"LON\", sf.col(\"LON\").cast(\"double\"))\n",
    "    \n",
    "    df = df.selectExpr(\"RPT_DIST_NO\", \"AREA\", \"LOCATION\", \"CROSS_STREET\", \"LAT\", \"LON\", \"last_updated\")\n",
    "    return df\n",
    "\n",
    "dp.create_auto_cdc_flow(\n",
    "    target=\"dim_location_lapd\",\n",
    "    source=\"gold_dim_location\",\n",
    "    keys=[\"RPT_DIST_NO\", \"AREA\"],\n",
    "    sequence_by=\"last_updated\",\n",
    "    ignore_null_updates=True\n",
    ")\n",
    "\n",
    "# DIM_VICTIM\n",
    "dp.create_streaming_table(\n",
    "    name=\"dim_victim_lapd\",\n",
    "    schema=\"\"\"\n",
    "        VICTIM_KEY bigint generated always as identity (start with 1 increment by 1),\n",
    "        VICTIM_AGE_RAW int,\n",
    "        VICTIM_SEX string,\n",
    "        VICTIM_DESCENT string,\n",
    "        VICTIM_AGE_GRP string,\n",
    "        VICTIM_DESCENT_DESC string,\n",
    "        last_updated timestamp\n",
    "    \"\"\",\n",
    "    table_properties={\"delta.enableChangeDataFeed\": \"true\"}\n",
    ")\n",
    "\n",
    "@dp.view()\n",
    "def gold_dim_victim():\n",
    "    df = spark.readStream.table('lapd_silver')\n",
    "    \n",
    "    # Descent mapping\n",
    "    descent_mapping = sf.create_map(\n",
    "        sf.lit(\"W\"), sf.lit(\"White\"),\n",
    "        sf.lit(\"H\"), sf.lit(\"Hispanic/Latin/Mexican\"),\n",
    "        sf.lit(\"B\"), sf.lit(\"Black/African American\"),\n",
    "        sf.lit(\"A\"), sf.lit(\"Other Asian\"),\n",
    "        sf.lit(\"C\"), sf.lit(\"Chinese\"),\n",
    "        sf.lit(\"D\"), sf.lit(\"Cambodian\"),\n",
    "        sf.lit(\"F\"), sf.lit(\"Filipino\"),\n",
    "        sf.lit(\"G\"), sf.lit(\"Guamanian\"),\n",
    "        sf.lit(\"I\"), sf.lit(\"American Indian/Alaskan Native\"),\n",
    "        sf.lit(\"J\"), sf.lit(\"Japanese\"),\n",
    "        sf.lit(\"K\"), sf.lit(\"Korean\"),\n",
    "        sf.lit(\"L\"), sf.lit(\"Laotian\"),\n",
    "        sf.lit(\"P\"), sf.lit(\"Pacific Islander\"),\n",
    "        sf.lit(\"S\"), sf.lit(\"Samoan\"),\n",
    "        sf.lit(\"U\"), sf.lit(\"Unknown\"),\n",
    "        sf.lit(\"V\"), sf.lit(\"Vietnamese\"),\n",
    "        sf.lit(\"Z\"), sf.lit(\"Asian Indian\"),\n",
    "        sf.lit(\"X\"), sf.lit(\"Other\")\n",
    "    )\n",
    "    \n",
    "    df = df.withColumn(\"VICTIM_AGE_RAW\", sf.col(\"VICT_AGE\").cast(\"int\"))\n",
    "    df = df.withColumn(\"VICTIM_SEX\", sf.col(\"VICT_SEX\"))\n",
    "    df = df.withColumn(\"VICTIM_DESCENT\", sf.col(\"VICT_DESCENT\"))\n",
    "    \n",
    "    df = df.withColumn(\"VICTIM_AGE_GRP\",\n",
    "        sf.when(sf.col(\"VICT_AGE\").between(0, 12), \"Child\")\n",
    "        .when(sf.col(\"VICT_AGE\").between(13, 19), \"Teen\")\n",
    "        .when(sf.col(\"VICT_AGE\").between(20, 35), \"Young Adult\")\n",
    "        .when(sf.col(\"VICT_AGE\").between(36, 55), \"Adult\")\n",
    "        .when(sf.col(\"VICT_AGE\") > 55, \"Senior\")\n",
    "        .otherwise(\"Unknown\"))\n",
    "    \n",
    "    df = df.withColumn(\"VICTIM_DESCENT_DESC\", descent_mapping[sf.col(\"VICT_DESCENT\")])\n",
    "\n",
    "    df = df.selectExpr(\"VICTIM_AGE_RAW\", \"VICTIM_SEX\", \"VICTIM_DESCENT\", \"VICTIM_AGE_GRP\", \"VICTIM_DESCENT_DESC\", \"last_updated\")\n",
    "    return df\n",
    "\n",
    "dp.create_auto_cdc_flow(\n",
    "    target=\"dim_victim_lapd\",\n",
    "    source=\"gold_dim_victim\",\n",
    "    keys=[\"VICTIM_AGE_RAW\", \"VICTIM_SEX\", \"VICTIM_DESCENT\"],\n",
    "    sequence_by=\"last_updated\",\n",
    "    ignore_null_updates=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "298593e0-ab48-4a5c-bcd7-b6accd57b4ee",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# FACT TABLE\n",
    "# ============================================\n",
    "dp.create_streaming_table(\n",
    "    name=\"fact_crime_incident_lapd\",\n",
    "    schema=\"\"\"\n",
    "        CRIME_INCIDENT_KEY bigint generated always as identity (start with 1 increment by 1),\n",
    "        DR_NO string,\n",
    "        AREA_KEY bigint,\n",
    "        LOCATION_KEY bigint,\n",
    "        CRIME_KEY bigint,\n",
    "        VICTIM_KEY bigint,\n",
    "        PREMISE_KEY bigint,\n",
    "        STATUS_KEY bigint,\n",
    "        WEAPON_KEY bigint,\n",
    "        TIME_OCC_KEY bigint,\n",
    "        DATE_RPT_KEY bigint,\n",
    "        DATE_OCC_KEY bigint,\n",
    "        MODUS_OPERANDI_KEY bigint,\n",
    "        INCIDENT_COUNT int,\n",
    "        created_at timestamp\n",
    "    \"\"\",\n",
    "    table_properties={\"delta.enableChangeDataFeed\": \"true\"}\n",
    ")\n",
    "\n",
    "@dp.append_flow(\n",
    "    target=\"fact_crime_incident_lapd\",\n",
    "    name=\"fact_crime_incident_flow\"\n",
    ")\n",
    "def gold_fact_crime_incident():\n",
    "    df = spark.sql(\"\"\"\n",
    "        SELECT\n",
    "            s.DR_NO,\n",
    "            COALESCE(a.AREA_KEY, -1) as AREA_KEY,\n",
    "            COALESCE(l.LOCATION_KEY, -1) as LOCATION_KEY,\n",
    "            COALESCE(c.CRIME_KEY, -1) as CRIME_KEY,\n",
    "            COALESCE(v.VICTIM_KEY, -1) as VICTIM_KEY,\n",
    "            COALESCE(p.PREMISE_KEY, -1) as PREMISE_KEY,\n",
    "            COALESCE(st.STATUS_KEY, -1) as STATUS_KEY,\n",
    "            COALESCE(w.WEAPON_KEY, -1) as WEAPON_KEY,\n",
    "            COALESCE(t.TIME_KEY, -1) as TIME_OCC_KEY,\n",
    "            COALESCE(dr.DATE_KEY, -1) as DATE_RPT_KEY,\n",
    "            COALESCE(do.DATE_KEY, -1) as DATE_OCC_KEY,\n",
    "            COALESCE(mo.MODUS_OPERANDI_KEY, -1) as MODUS_OPERANDI_KEY,\n",
    "            1 as INCIDENT_COUNT,\n",
    "            s.last_updated as created_at\n",
    "        FROM STREAM(lapd_silver) s\n",
    "        LEFT JOIN dim_area_lapd a ON s.AREA = a.AREA_CODE\n",
    "        LEFT JOIN dim_crime_lapd c ON s.CRM_CD_1 = c.CRM_CD_1\n",
    "        LEFT JOIN dim_premise_lapd p ON s.PREMIS_CD = p.PREMIS_CD\n",
    "        LEFT JOIN dim_status_lapd st ON s.STATUS = st.STATUS_CODE\n",
    "        LEFT JOIN dim_weapon_lapd w ON s.WEAPON_USED_CD = w.WEAPON_USED_CD\n",
    "        LEFT JOIN dim_time_lapd t ON s.TIME_OCC_HOUR = t.HOUR\n",
    "        LEFT JOIN dim_date_lapd dr ON s.DATE_RPTD = dr.FULL_DATE\n",
    "        LEFT JOIN dim_date_lapd do ON s.DATE_OCC = do.FULL_DATE\n",
    "        LEFT JOIN dim_modus_operandi_lapd mo ON s.MOCODES = mo.MOCODES\n",
    "        LEFT JOIN dim_location_lapd l ON s.RPT_DIST_NO = l.RPT_DIST_NO AND s.AREA = l.AREA\n",
    "        LEFT JOIN dim_victim_lapd v ON cast(s.VICT_AGE as int) = v.VICTIM_AGE_RAW \n",
    "            AND s.VICT_SEX = v.VICTIM_SEX \n",
    "            AND s.VICT_DESCENT = v.VICTIM_DESCENT\n",
    "    \"\"\")\n",
    "    return df"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "LAPD_TeamAssignment",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
